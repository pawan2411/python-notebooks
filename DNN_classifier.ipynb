{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN based all class classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x-R41HZvN8D9",
    "outputId": "3c81baea-5146-4035-932e-fd4479add70d"
   },
   "outputs": [],
   "source": [
    "!pip install -U keras\n",
    "!pip install -U tensorflow\n",
    "!pip install -U pandas\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWf76aatMer5"
   },
   "source": [
    "#### Load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NpFgQHHOMer6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input,Embedding,Dense,Flatten,concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avTPIyF5Mer7"
   },
   "source": [
    "### Flowchart of the problem approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "gCu9qViRMer7",
    "outputId": "95634658-ba2a-40a3-c7e8-b64f1bdb7fd0"
   },
   "outputs": [],
   "source": [
    "Image(filename='keras_func_api.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKyagaZXMer8"
   },
   "source": [
    "## Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3p3KqFvfMer8"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",delimiter=\",\",encoding='utf-8', index_col=False)\n",
    "test_csv = pd.read_csv(\"test.csv\",delimiter=\",\",encoding='utf-8', index_col=False)\n",
    "valid_csv = pd.read_csv(\"valid.csv\",delimiter=\",\",encoding='utf-8', index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oblb2Xp5MesC"
   },
   "source": [
    "#### add length feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ygFatFteMesC"
   },
   "outputs": [],
   "source": [
    "data['Text_len'] = [len(i) for i in data[\"Text\"]]\n",
    "test_csv['Text_len'] = [len(i) for i in test_csv[\"Text\"]]\n",
    "valid_csv['Text_len'] = [len(i) for i in valid_csv[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DXoaEhPBMesF"
   },
   "outputs": [],
   "source": [
    "target_attr = 'ParagraphType'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEM_PkbHMesH"
   },
   "source": [
    "#### segregate the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FZ7WE0MkMesH"
   },
   "outputs": [],
   "source": [
    "data_numerical_train, data_string_train, Y_train = data[['Text_len']], data[[\"Text\"]],data[[\"ParagraphType\"]]\n",
    "data_numerical_valid, data_string_valid, Y_valid = valid_csv[['Text_len']], valid_csv[[\"Text\"]],valid_csv[[\"ParagraphType\"]]\n",
    "data_numerical_test, data_string_test, Y_test = test_csv[['Text_len']], test_csv[[\"Text\"]],test_csv[[\"ParagraphType\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf6QOSrdMesK"
   },
   "source": [
    "#### Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpCLxDXOMesK",
    "outputId": "0d658719-b2c3-4e86-9745-77271c26bc26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TI', 'AB', 'H1', 'PA', 'H2', 'LI', 'BY', 'H3', 'HA', 'CO'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ParagraphType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GKeH-Xj8MesL"
   },
   "outputs": [],
   "source": [
    "no_of_levels=len(data['ParagraphType'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLE4zhEDMesL"
   },
   "source": [
    "Since there are 10 different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rc3TRzJPMesL"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1YMueFZOMesL"
   },
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CegeeULQPmyq"
   },
   "outputs": [],
   "source": [
    "Y_train = pd.DataFrame(Y_train)\n",
    "Y_test = pd.DataFrame(Y_test)\n",
    "Y_valid = pd.DataFrame(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kYgonijFMesM"
   },
   "outputs": [],
   "source": [
    "OneHotEncoder = onehotencoder.fit(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "J-5RzIECMesM"
   },
   "outputs": [],
   "source": [
    "OneHotEncoder_target_train = OneHotEncoder.transform(Y_train).toarray()\n",
    "OneHotEncoder_target_test = OneHotEncoder.transform(Y_test).toarray()\n",
    "OneHotEncoder_target_valid = OneHotEncoder.transform(Y_valid).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlYC5QNRMesM",
    "outputId": "0228632c-171b-4fa8-b6f9-4c476acdd6c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4985, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotEncoder_target_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuID46hVMesN",
    "outputId": "512d7031-7480-4ec3-84e3-68c01f805eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OneHotEncoder_target_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TN-OUrkMesO"
   },
   "source": [
    "### Pre-Processing of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuBJQME2MesR"
   },
   "source": [
    "#### I used 1000 as max length of the paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05FppUJLMesR"
   },
   "source": [
    "#### Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPP4Mrp4MesS",
    "outputId": "061d7863-71a2-4f83-fcbb-67feb0aa01a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232531 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data_string_train['Text'])\n",
    "review_text_train = tokenizer.texts_to_sequences(data_string_train['Text'])\n",
    "review_text_test = tokenizer.texts_to_sequences(data_string_test['Text'])\n",
    "review_text_valid = tokenizer.texts_to_sequences(data_string_valid['Text'])\n",
    "\n",
    "word_index_review_text = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index_review_text))\n",
    "NUM_WORDS_REVIEW_TEXT = len(word_index_review_text)+1\n",
    "\n",
    "review_text_seq_train = pad_sequences(review_text_train, maxlen=1000)\n",
    "review_text_seq_test = pad_sequences(review_text_test, maxlen=1000)\n",
    "review_text_seq_valid = pad_sequences(review_text_valid, maxlen=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Nf6W64HMesS"
   },
   "source": [
    "###### Load the GloVe word embedding file into memory as a dictionary of word to embedding array.\n",
    "\n",
    "__Note__: Filter the embedding for the unique words in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWv-xe_UMesS",
    "outputId": "c3a13c7b-8581-467c-fb43-0fd1e32c99da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbjdFlLoMesT"
   },
   "source": [
    "#### Next, create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "\n",
    "#### The result is a matrix of weights only for words we will see during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8fm8OVgMesT"
   },
   "source": [
    "#### Also count the number of words not present in the glove to decide whether we need to train or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Wrx2zvknMesT"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "review_embedding_matrix = np.zeros((NUM_WORDS_REVIEW_TEXT,200))\n",
    "review_word_not_in_glove_count = 0\n",
    "review_word_not_in_glove =[]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        review_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        review_word_not_in_glove.append(word)\n",
    "        review_word_not_in_glove_count = review_word_not_in_glove_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLYOPlefMesT",
    "outputId": "cbcac267-41ab-4bc9-fd98-7423fd4696db"
   },
   "outputs": [],
   "source": [
    "print(review_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnqDbjstMesU",
    "outputId": "f8ed3919-83e4-4034-dcf8-f237b6d55f7b"
   },
   "outputs": [],
   "source": [
    "print(review_word_not_in_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qNXWJ3IMesU",
    "outputId": "0f94f852-a96f-424d-efcf-f01ee13d8d43"
   },
   "outputs": [],
   "source": [
    "print(review_word_not_in_glove_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDW55GRkMesY"
   },
   "source": [
    "## 3.a 10 class classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRpG4zqSMesZ"
   },
   "source": [
    "#### Embedding layer for Paragraph Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khhjnNo0MesZ"
   },
   "source": [
    "#### If there are more than one word in the training data which are not present in Glove then train the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cat_inputs = Input(shape=(data_numerical_train.shape[1],),name='num_cat_inputs')\n",
    "out_num_cat = Dense(64, activation='relu')(num_cat_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zoR0QEXxMesZ"
   },
   "outputs": [],
   "source": [
    "text_input= Input(shape=(1000,),name='text_input')\n",
    "\n",
    "if (review_word_not_in_glove_count<=1):\n",
    "    text_embed = Embedding(input_dim=NUM_WORDS_REVIEW_TEXT,output_dim=200,weights=[review_embedding_matrix],trainable=False)(text_input)\n",
    "else:\n",
    "    text_embed = Embedding(input_dim=NUM_WORDS_REVIEW_TEXT,output_dim=200,weights=[review_embedding_matrix],trainable=True)(text_input)\n",
    "\n",
    "out_text = Flatten()(text_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOgoyL11Mesa"
   },
   "source": [
    "#### Concatenate the output of above layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "v7D6eob2Mesa"
   },
   "outputs": [],
   "source": [
    "concatenated = concatenate([out_num_cat,out_text])\n",
    "X = Dense(8, activation='relu')(concatenated)\n",
    "final_out = Dense(no_of_levels, activation='softmax')(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ecAjyVz5Mesa"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[num_cat_inputs,text_input], outputs=final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCukaLaTMesa",
    "outputId": "b7f7db21-f070-422e-c8ff-b60adaf0cd72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text_input (InputLayer)         [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "num_cat_inputs (InputLayer)     [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 200)    46506400    text_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           128         num_cat_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200000)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 200064)       0           dense[0][0]                      \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            1600520     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           90          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 48,107,138\n",
      "Trainable params: 48,107,138\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Uj0sYLxEMesb"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NceRufnzMesb",
    "outputId": "5042130a-fa72-4fb6-af54-3257e50f02b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6250/6250 [==============================] - 276s 44ms/step - loss: 0.9077 - accuracy: 0.7031 - val_loss: 0.7985 - val_accuracy: 0.7240\n",
      "Epoch 2/10\n",
      "6250/6250 [==============================] - 278s 44ms/step - loss: 0.7693 - accuracy: 0.7369 - val_loss: 0.7536 - val_accuracy: 0.7356\n",
      "Epoch 3/10\n",
      "6250/6250 [==============================] - 285s 46ms/step - loss: 0.7361 - accuracy: 0.7459 - val_loss: 0.7360 - val_accuracy: 0.7413\n",
      "Epoch 4/10\n",
      "6250/6250 [==============================] - 279s 45ms/step - loss: 0.7173 - accuracy: 0.7507 - val_loss: 0.7243 - val_accuracy: 0.7434\n",
      "Epoch 5/10\n",
      "6250/6250 [==============================] - 288s 46ms/step - loss: 0.7031 - accuracy: 0.7546 - val_loss: 0.7108 - val_accuracy: 0.7498\n",
      "Epoch 6/10\n",
      "6250/6250 [==============================] - 275s 44ms/step - loss: 0.6912 - accuracy: 0.7581 - val_loss: 0.7024 - val_accuracy: 0.7524\n",
      "Epoch 7/10\n",
      "6250/6250 [==============================] - 277s 44ms/step - loss: 0.6819 - accuracy: 0.7608 - val_loss: 0.6968 - val_accuracy: 0.7540\n",
      "Epoch 8/10\n",
      "6250/6250 [==============================] - 276s 44ms/step - loss: 0.6743 - accuracy: 0.7633 - val_loss: 0.6920 - val_accuracy: 0.7552\n",
      "Epoch 9/10\n",
      "6250/6250 [==============================] - 279s 45ms/step - loss: 0.6679 - accuracy: 0.7653 - val_loss: 0.6870 - val_accuracy: 0.7602\n",
      "Epoch 10/10\n",
      "6250/6250 [==============================] - 277s 44ms/step - loss: 0.6623 - accuracy: 0.7670 - val_loss: 0.6825 - val_accuracy: 0.7611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f66c3ac0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_numerical_train,review_text_seq_train], \n",
    "          y=OneHotEncoder_target_train, \n",
    "          epochs=10,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b accuracy and confusion matrix on validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sk3BR1_qMesb",
    "outputId": "30330ca3-fe96-4893-c9ca-74e9fca79ae7"
   },
   "outputs": [],
   "source": [
    "pred_valid=model.predict([data_numerical_valid,review_text_seq_valid])\n",
    "# model.evaluate([data_numerical_valid,review_text_seq_valid], \n",
    "#                y=OneHotEncoder_target_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.742952603885321\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>599</td>\n",
       "      <td>0</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>317</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7610</td>\n",
       "      <td>96</td>\n",
       "      <td>654</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>8407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2089</td>\n",
       "      <td>284</td>\n",
       "      <td>1081</td>\n",
       "      <td>30</td>\n",
       "      <td>198</td>\n",
       "      <td>3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>99</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>919</td>\n",
       "      <td>197</td>\n",
       "      <td>5802</td>\n",
       "      <td>1356</td>\n",
       "      <td>163</td>\n",
       "      <td>8449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>525</td>\n",
       "      <td>249</td>\n",
       "      <td>1778</td>\n",
       "      <td>22625</td>\n",
       "      <td>216</td>\n",
       "      <td>25426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>277</td>\n",
       "      <td>184</td>\n",
       "      <td>862</td>\n",
       "      <td>3</td>\n",
       "      <td>403</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>3</td>\n",
       "      <td>510</td>\n",
       "      <td>11783</td>\n",
       "      <td>1177</td>\n",
       "      <td>10689</td>\n",
       "      <td>24644</td>\n",
       "      <td>1177</td>\n",
       "      <td>49983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  0    1      3     4      7      8     9    All\n",
       "Actual                                                   \n",
       "0          0    0      2     0      8    599     0    609\n",
       "1          0  411     18    40    317      0   120    906\n",
       "2          0    0     16     7     81     15     3    122\n",
       "3          0    2   7610    96    654     15    30   8407\n",
       "4          0    7   2089   284   1081     30   198   3689\n",
       "5          0    1    248    99    106      1    23    478\n",
       "6          0    0     79    21      0      0    21    121\n",
       "7          0   12    919   197   5802   1356   163   8449\n",
       "8          3   30    525   249   1778  22625   216  25426\n",
       "9          0   47    277   184    862      3   403   1776\n",
       "All        3  510  11783  1177  10689  24644  1177  49983"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: ', accuracy_score(OneHotEncoder_target_valid.argmax(axis=1), pred_valid.argmax(axis=1)))\n",
    "pd.crosstab(OneHotEncoder_target_valid.argmax(axis=1), pred_valid.argmax(axis=1), rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.c Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I choose a feed forward NN with two inputs 1. the length of paragraph plus 2. word embedding with 1000 words max paragraphs, smaller and larger paragraphs are padded, i choose 1000 because almost all paragraph lengths are covered with this max number. As we are using word embeddings this also solves major problem ie most similar words are grouped in similar spaces in embedding hence results improve. \n",
    "\n",
    "#### Improvements\n",
    "We can do hyper parameter optimization, we can also do sampling and reduce the biasness in input samples for each class.\n",
    "I do not think use of more complex networks like LSTMs, transformers etc will improve on performance much because most important features are length of paragraph and some important keywords like \"refrences, bibliography\" etc and their occurences in each text.\n",
    "#### That is the reason bag of words classification model in Question 2 for binary classifier worked very good.\n",
    "#### Hence on the same lines if I would have got time i would have done bag of words using Glove, this would have improved more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.d new test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "t3K3wK7BMesc"
   },
   "outputs": [],
   "source": [
    "pred=model.predict([data_numerical_test,review_text_seq_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuLhwGLhMesc",
    "outputId": "b9892ab9-2a95-4cfe-b950-7d1a3f7ff84b"
   },
   "outputs": [],
   "source": [
    "y_classes=pred.argmax(axis=1)\n",
    "y_classes\n",
    "test_preds = onehotencoder.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results_for_test = [item for sublist in test_preds  for item in sublist]\n",
    "test_csv['ParagraphType'] = new_results_for_test\n",
    "test_csv = test_csv.drop('Text_len', 1)\n",
    "test_csv.to_csv(\"test.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "womens_clothing_e_commerce_reviews.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
